{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predict_best_answer_GAV.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1y4KbfodpkVNKfLL8_E7xkevocJxh9bU0",
      "authorship_tag": "ABX9TyOKqzf7X/WgwbwSWe/rEgMm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pgauravv/NLP_Predict_best_answer/blob/master/Predict_best_answer_GAV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5njHy5QPB0Wz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# my thanks go to Baptiste + DP\n",
        "# A-P\n",
        "import os\n",
        "import numpy as np\n",
        "from scipy.sparse import csc_matrix, save_npz, load_npz\n",
        "import re\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "from spacy.lang.en import English\n",
        "from datetime import datetime\n",
        "from gensim.corpora import Dictionary\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "\n",
        "\n",
        "#import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAvqMdDvHnTI",
        "colab_type": "code",
        "outputId": "77d4f967-f482-4691-cbd6-71e11029f69d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "root_path = '/content/drive/My Drive/NLP_Final_Project'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIiBBWITCwQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# 18 fields expected.\n",
        "field_names = ['id', 'type', 'is_best_answer', 'topic_id', 'parent_id', 'votes',\n",
        "               'title', 'content', 'member', 'category', 'state', 'is_solved', 'num_answers',\n",
        "               'country', 'date', 'last_answer_date', 'author_crc', 'visits']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHKWoESEC6Nz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "txt_path = \"/content/drive/My Drive/NLP_Final_Project/QA/export-forums_en.csv\"\n",
        "entity_path = \"/content/drive/My Drive/NLP_Final_Project/QA/export-forums_en.pickle\"\n",
        "csv_path = \"/content/drive/My Drive/NLP_Final_Project/QA/export-forums_en.format.csv\"\n",
        "data_path = \"/content/drive/My Drive/NLP_Final_Project/QA/\"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02DYLFeGEMvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def format_entities():\n",
        "    '''\n",
        "    Read the raw data, format the list of entities, serialize them.\n",
        "    '''\n",
        "\n",
        "    def build_entities(txt_path, max_entities=None):\n",
        "        '''\n",
        "        Return a list of structured entities from raw txt file.\n",
        "        '''\n",
        "        # Read text file.\n",
        "        with open(txt_path, 'r', encoding='utf8') as f:\n",
        "            # Entities and current entity.\n",
        "            entities, entity = [], {}\n",
        "            # Entity values might be split over lines\n",
        "            field_counter = 0\n",
        "            # Process lines\n",
        "            for line in f:\n",
        "                # Prepare line\n",
        "                line = line.replace(\"\\\\N\", '\"unkwown\"')\n",
        "                # Char start for extracted value.\n",
        "                char_start = 1\n",
        "                # Find values separators\n",
        "                field_index = [m.start() for m in re.finditer('\",\"', line)]\n",
        "                # Browse value separators.\n",
        "                for index in field_index:\n",
        "                    # Extract in between value.\n",
        "                    value = line[char_start:index]\n",
        "                    # Update start index.\n",
        "                    char_start = index + 3\n",
        "                    # Update field counter.\n",
        "                    field_counter += 1\n",
        "                    # Update entity value.\n",
        "                    try:\n",
        "                        entity[field_names[field_counter-1]] += value\n",
        "                    except KeyError:\n",
        "                        entity[field_names[field_counter-1]] = value\n",
        "                    except IndexError:\n",
        "                        entity = {}\n",
        "                        field_counter = 0\n",
        "                # Content string is split.\n",
        "                if field_counter == 7 and len(field_index) > 0:\n",
        "                    entity[field_names[7]] = line[field_index[-1]:]\n",
        "                    continue\n",
        "                # Next content string.\n",
        "                if field_counter == 7 and len(field_index) == 0:\n",
        "                    entity[field_names[7]] += line\n",
        "                    continue\n",
        "                # Next entity.\n",
        "                if len(entity) == 17:\n",
        "                    field_counter = 0\n",
        "                    entities.append(entity)\n",
        "                    entity = {}\n",
        "                    if max_entities is not None:\n",
        "                        if len(entities) > max_entities:\n",
        "                            return entities\n",
        "        return entities\n",
        "\n",
        "    # Write entities on disk.\n",
        "    with open(entity_path, 'wb') as f:\n",
        "        pickle.dump(build_entities(txt_path=txt_path, max_entities=None), f)\n",
        "        print(f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp_2EPP-EYZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def entities_to_csv():\n",
        "    '''\n",
        "    Format entities to csv.\n",
        "    '''\n",
        "    with open(entity_path, 'rb') as obj:\n",
        "        entities = pickle.load(obj)\n",
        "    x = pd.DataFrame(entities)\n",
        "    x.to_csv(csv_path)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0TlihMJ_Pvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def build_index():\n",
        "    '''\n",
        "    Build index and write.\n",
        "    '''\n",
        "\n",
        "    # Read entities.\n",
        "    with open(entity_path, 'rb') as obj:\n",
        "        entities = pickle.load(obj)\n",
        "\n",
        "    # Build indexes\n",
        "    user_index, question_index, answer_index, comment_index = {}, {}, {}, {}\n",
        "    for e in entities:\n",
        "        # Author\n",
        "        if e['author_crc'] not in user_index:\n",
        "            user_index[e['author_crc']] = len(user_index)\n",
        "        # Questions\n",
        "        if e['type'] == 'Q':\n",
        "            if e['id'] not in question_index:\n",
        "                question_index[e['id']] = len(question_index)\n",
        "        # Answers\n",
        "        if e['type'] == 'A':\n",
        "            if e['id'] not in answer_index:\n",
        "                answer_index[e['id']] = len(answer_index)\n",
        "        # Answers\n",
        "        if e['type'] == 'C':\n",
        "            if e['id'] not in comment_index:\n",
        "                comment_index[e['id']] = len(comment_index)\n",
        "\n",
        "    # Write index.\n",
        "    with open(os.path.join(data_path, 'user.index'), 'wb') as f:\n",
        "        pickle.dump(user_index, f)\n",
        "    with open(os.path.join(data_path, 'question.index'), 'wb') as f:\n",
        "        pickle.dump(question_index, f)\n",
        "    with open(os.path.join(data_path, 'answer.index'), 'wb') as f:\n",
        "        pickle.dump(answer_index, f)\n",
        "    with open(os.path.join(data_path, 'comment.index'), 'wb') as f:\n",
        "        pickle.dump(comment_index, f)\n",
        "\n",
        "    # Logs.\n",
        "    print(\"Entities: \", len(entities))\n",
        "    print(\"Users: \", len(user_index))\n",
        "    print(\"Questions: \", len(question_index))\n",
        "    print(\"Answers: \", len(answer_index))\n",
        "    print(\"Comments: \", len(comment_index))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVuGwFVj_T0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def read_indexes():\n",
        "    '''\n",
        "    Return user, question, answer and comment index.\n",
        "    '''\n",
        "    with open(os.path.join(data_path, 'user.index'), 'rb') as obj:\n",
        "        user_index = pickle.load(obj)\n",
        "    with open(os.path.join(data_path, 'question.index'), 'rb') as obj:\n",
        "        question_index = pickle.load(obj)\n",
        "    with open(os.path.join(data_path, 'answer.index'), 'rb') as obj:\n",
        "        answer_index = pickle.load(obj)\n",
        "    with open(os.path.join(data_path, 'comment.index'), 'rb') as obj:\n",
        "        comment_index = pickle.load(obj)\n",
        "    return user_index, question_index, answer_index, comment_index\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyPyrvQH_XJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def build_relations():\n",
        "    '''\n",
        "    Build UQ, UA, QA, UC, CA relations.\n",
        "    '''\n",
        "\n",
        "    # Read entities.\n",
        "    with open(entity_path, 'rb') as obj:\n",
        "        entities = pickle.load(obj)\n",
        "\n",
        "    # Read indexes\n",
        "    user_index, question_index, answer_index, comment_index = read_indexes()\n",
        "\n",
        "    # Relations\n",
        "    uq = []\n",
        "    ua = []\n",
        "    qa = []\n",
        "    uc = []\n",
        "    ca = []\n",
        "\n",
        "    # Browse elements.\n",
        "    for e in entities:\n",
        "\n",
        "        # UQ\n",
        "        if e['type'] == 'Q':\n",
        "            u = user_index[e['author_crc']]\n",
        "            q = question_index[e['id']]\n",
        "            t = datetime.utcfromtimestamp(int(e['date'])).strftime('%Y-%m-%d %H:%M:%S')\n",
        "            uq.append((u,q,t))\n",
        "\n",
        "        # UA, QA\n",
        "        if e['type'] == 'A':\n",
        "            u = user_index[e['author_crc']]\n",
        "            a = answer_index[e['id']]\n",
        "            q = question_index[e['parent_id']]\n",
        "            t = datetime.utcfromtimestamp(int(e['date'])).strftime('%Y-%m-%d %H:%M:%S')\n",
        "            ua.append((u,a,t))\n",
        "            qa.append((q,a,t))\n",
        "\n",
        "        # UC, CA\n",
        "        if e['type'] == 'C':\n",
        "            try:\n",
        "                u = user_index[e['author_crc']]\n",
        "                c = comment_index[e['id']]\n",
        "                a = answer_index[e['parent_id']]\n",
        "                t = datetime.utcfromtimestamp(int(e['date'])).strftime('%Y-%m-%d %H:%M:%S')\n",
        "                uc.append((u,c,t))\n",
        "                ca.append((c,a,t))\n",
        "            except KeyError:\n",
        "                continue\n",
        "\n",
        "    # Write relations.\n",
        "    with open(os.path.join(data_path, 'uq.rel'), 'wb') as f:\n",
        "        pickle.dump(uq, f)\n",
        "    # Write relations.\n",
        "    with open(os.path.join(data_path, 'ua.rel'), 'wb') as f:\n",
        "        pickle.dump(ua, f)\n",
        "    # Write relations.\n",
        "    with open(os.path.join(data_path, 'qa.rel'), 'wb') as f:\n",
        "        pickle.dump(qa, f)\n",
        "    # Write relations.\n",
        "    with open(os.path.join(data_path, 'uc.rel'), 'wb') as f:\n",
        "        pickle.dump(uc, f)\n",
        "    # Write relations.\n",
        "    with open(os.path.join(data_path, 'ca.rel'), 'wb') as f:\n",
        "        pickle.dump(ca, f)\n",
        "\n",
        "    # Logs.\n",
        "    print(\"uq: \", len(uq))\n",
        "    print(\"ua: \", len(ua))\n",
        "    print(\"qa: \", len(qa))\n",
        "    print(\"uc: \", len(uc))\n",
        "    print(\"ca: \", len(ca))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pz58EOaT_alq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def build_vocabulary_and_corpus():\n",
        "    '''\n",
        "    Build the vocabularies and stem sequences for each type of entities.\n",
        "    '''\n",
        "\n",
        "    # Vocabulary (same for question and answers)\n",
        "    v = Dictionary()\n",
        "\n",
        "    # Stemmer.\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Tokenizer.\n",
        "    tokenizer = TweetTokenizer()\n",
        "\n",
        "    # Read indexes\n",
        "    user_index, question_index, answer_index, comment_index = read_indexes()\n",
        "\n",
        "    # Question, answer\n",
        "    q = {}\n",
        "    a = {}\n",
        "\n",
        "    # Read entities.\n",
        "    with open(entity_path, 'rb') as obj:\n",
        "        entities = pickle.load(obj)\n",
        "\n",
        "    # Browse question and answers to first build vocabulary.\n",
        "    for e in entities:\n",
        "        # Question or answer.\n",
        "        if e['type'] == 'Q' or e['type'] == 'A':\n",
        "            # String content.\n",
        "            title = str(e['title']).encode('utf-8').lower()\n",
        "            content = str(e['content']).encode('utf-8').lower()\n",
        "            # Tokenize\n",
        "            d = tokenizer.tokenize(title + content)\n",
        "            # Stem word\n",
        "            d = [stemmer.stem(s) for s in d]\n",
        "            # Process vocabulary.\n",
        "            v.add_documents([d])\n",
        "            # Question\n",
        "            if e['type'] == 'Q':\n",
        "                q[question_index[e['id']]] = d\n",
        "            # Answer\n",
        "            if e['type'] == 'A':\n",
        "                a[answer_index[e['id']]] = d\n",
        "\n",
        "    # Write question corpus.\n",
        "    with open(os.path.join(data_path, 'q.corpus'), 'wb') as f:\n",
        "        pickle.dump(q, f)\n",
        "\n",
        "    # Write answer corpus.\n",
        "    with open(os.path.join(data_path, 'a.corpus'), 'wb') as f:\n",
        "        pickle.dump(a, f)\n",
        "\n",
        "    # Write to analyse.\n",
        "    v.filter_extremes(no_below=1000, keep_n=10000)\n",
        "    v.compactify()\n",
        "    v.save(os.path.join(data_path, \"raw_vocabulary.gensim\"))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7Z17lAniipn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "from scipy import spatial\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_xnLif3_dS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def build_embeddings():\n",
        "  # # Load the spacy model that you have installed\n",
        "  # nlp = spacy.load('en_core_web_md')\n",
        "  # # process a sentence using the model\n",
        "  # doc = nlp(\"This is some text that I am processing with Spacy\")\n",
        "  # # It's that simple - all of the vectors and words are assigned after this point\n",
        "  # # Get the vector for 'text':\n",
        "  # doc[3].vector\n",
        "  # # Get the mean vector for the entire sentence (useful for sentence classification etc.)\n",
        "  # doc.vector\n",
        "    print(\"Todo: your code goes here\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVKuvLQU_elL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def build_models():\n",
        "    print(\"Todo: your code goes here\")    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmZRdCf6_fUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# if __name__ == '__main__':\n",
        "    \n",
        "#     format_entities()\n",
        "#     build_index()\n",
        "#     build_relations()\n",
        "#     build_vocabulary_and_corpus()\n",
        "\n",
        "#     # build_embeddings()\n",
        "#     # build_models()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OO2wlkUhnWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " with open(os.path.join(data_path, 'uq.rel'), 'rb') as obj:\n",
        "   uq = pickle.load(obj)\n",
        "\n",
        "with open(os.path.join(data_path, 'ua.rel'), 'rb') as obj:\n",
        "  ua = pickle.load(obj)\n",
        "\n",
        "with open(os.path.join(data_path, 'qa.rel'), 'rb') as obj:\n",
        "  qa = pickle.load(obj)\n",
        "\n",
        "with open(os.path.join(data_path, 'uc.rel'), 'rb') as obj:\n",
        "  ca = pickle.load(obj)\n",
        "    \n",
        "with open(os.path.join(data_path, 'ca.rel'), 'rb') as obj:\n",
        "  ca = pickle.load(obj)\n",
        "\n",
        "with open(os.path.join(data_path, 'q.corpus'), 'rb') as obj:\n",
        "  q_corpus = pickle.load(obj)\n",
        "    \n",
        "with open(os.path.join(data_path, 'a.corpus'), 'rb') as obj:\n",
        "  a_corpus = pickle.load(obj)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-qQA2Xnl0sA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOH22KXZuuqI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_q_a=pd.read_csv(csv_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtZJthqou1xE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "0225ad86-2577-45d3-eac5-d723f7c3a5e6"
      },
      "source": [
        "df_q_a.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>type</th>\n",
              "      <th>is_best_answer</th>\n",
              "      <th>topic_id</th>\n",
              "      <th>parent_id</th>\n",
              "      <th>votes</th>\n",
              "      <th>title</th>\n",
              "      <th>content</th>\n",
              "      <th>member</th>\n",
              "      <th>category</th>\n",
              "      <th>state</th>\n",
              "      <th>is_solved</th>\n",
              "      <th>num_answers</th>\n",
              "      <th>country</th>\n",
              "      <th>date</th>\n",
              "      <th>last_answer_date</th>\n",
              "      <th>author_crc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>69561</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>36102</td>\n",
              "      <td>0</td>\n",
              "      <td>Windows Vista to XP Downgrading/Reformat?</td>\n",
              "      <td>&lt;a href='http://paparadit.blogspot.com/2007/06...</td>\n",
              "      <td>0</td>\n",
              "      <td>136</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>180</td>\n",
              "      <td>IN</td>\n",
              "      <td>1234889592</td>\n",
              "      <td>1306387514</td>\n",
              "      <td>865612499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>587308</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>17183</td>\n",
              "      <td>0</td>\n",
              "      <td>Windows Vista to XP Downgrading/Reformat?</td>\n",
              "      <td>but my does not have where to off the SATA. wh...</td>\n",
              "      <td>0</td>\n",
              "      <td>136</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>180</td>\n",
              "      <td>unkwown</td>\n",
              "      <td>1304428703</td>\n",
              "      <td>1306387514</td>\n",
              "      <td>1949389026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>587682</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>8981</td>\n",
              "      <td>0</td>\n",
              "      <td>Windows Vista to XP Downgrading/Reformat?</td>\n",
              "      <td>windows 7 (very similar to windows vista) won'...</td>\n",
              "      <td>0</td>\n",
              "      <td>136</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>180</td>\n",
              "      <td>unkwown</td>\n",
              "      <td>1304540935</td>\n",
              "      <td>1306387514</td>\n",
              "      <td>1017830945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>614161</td>\n",
              "      <td>C</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>6930</td>\n",
              "      <td>0</td>\n",
              "      <td>Windows Vista to XP Downgrading/Reformat?</td>\n",
              "      <td>any body in this forum having problem of doung...</td>\n",
              "      <td>0</td>\n",
              "      <td>136</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>180</td>\n",
              "      <td>unkwown</td>\n",
              "      <td>1316594311</td>\n",
              "      <td>1306387514</td>\n",
              "      <td>3324437120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>Q</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Windows Vista to XP Downgrading/Reformat?</td>\n",
              "      <td>\",\"Hello,\\n\\\\n&lt;br&gt;I bought a new laptop with W...</td>\n",
              "      <td>0</td>\n",
              "      <td>136</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>180</td>\n",
              "      <td>FR</td>\n",
              "      <td>1199900147</td>\n",
              "      <td>1306387514</td>\n",
              "      <td>418598184</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0      id type  ...        date  last_answer_date  author_crc\n",
              "0           0   69561    C  ...  1234889592        1306387514   865612499\n",
              "1           1  587308    C  ...  1304428703        1306387514  1949389026\n",
              "2           2  587682    C  ...  1304540935        1306387514  1017830945\n",
              "3           3  614161    C  ...  1316594311        1306387514  3324437120\n",
              "4           4       4    Q  ...  1199900147        1306387514   418598184\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DapNGEJFJZuN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "13fc94a9-0e69-4512-fbf5-80d752ad30ac"
      },
      "source": [
        "df_q_a.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(715516, 18)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMtcjmPX0kRc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "6fc649f2-f326-43d0-dc3a-f4400ade2d7b"
      },
      "source": [
        "df_q_a.info()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 715516 entries, 0 to 715515\n",
            "Data columns (total 18 columns):\n",
            " #   Column            Non-Null Count   Dtype \n",
            "---  ------            --------------   ----- \n",
            " 0   Unnamed: 0        715516 non-null  int64 \n",
            " 1   id                715516 non-null  int64 \n",
            " 2   type              715516 non-null  object\n",
            " 3   is_best_answer    715516 non-null  int64 \n",
            " 4   topic_id          715516 non-null  int64 \n",
            " 5   parent_id         715516 non-null  int64 \n",
            " 6   votes             715516 non-null  int64 \n",
            " 7   title             715509 non-null  object\n",
            " 8   content           715378 non-null  object\n",
            " 9   member            715516 non-null  int64 \n",
            " 10  category          715516 non-null  int64 \n",
            " 11  state             715516 non-null  int64 \n",
            " 12  is_solved         715516 non-null  int64 \n",
            " 13  num_answers       715516 non-null  int64 \n",
            " 14  country           715512 non-null  object\n",
            " 15  date              715516 non-null  int64 \n",
            " 16  last_answer_date  715516 non-null  int64 \n",
            " 17  author_crc        715516 non-null  int64 \n",
            "dtypes: int64(14), object(4)\n",
            "memory usage: 98.3+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRf8PK7F0xE-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "95d152f6-5ffc-4fbe-f4e3-6333acccba20"
      },
      "source": [
        "df_q_a.is_best_answer.value_counts()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    686077\n",
              "1     29439\n",
              "Name: is_best_answer, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxihEElF1A1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# Create our list of punctuation marks\n",
        "punctuations = string.punctuation\n",
        "\n",
        "# Create our list of stopwords\n",
        "nlp = spacy.load('en')\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "parser = English()\n",
        "\n",
        "# Creating our tokenizer function\n",
        "def spacy_tokenizer(sentence):\n",
        "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
        "    mytokens = parser(sentence)\n",
        "\n",
        "    # Lemmatizing each token and converting each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Removing stop words\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    # return preprocessed list of tokens\n",
        "    return mytokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThuUpdLs1UHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Custom transformer using spaCy\n",
        "class predictors(TransformerMixin):\n",
        "    def transform(self, X, **transform_params):\n",
        "        # Cleaning Text\n",
        "        return [clean_text(text) for text in X]\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {}\n",
        "\n",
        "# Basic function to clean the text\n",
        "def clean_text(text):\n",
        "    # Removing spaces and converting text into lowercase\n",
        "    return text.strip().lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO0G4WCy1aM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glAjJeSM1eET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHwiG7RS2LIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_q_a=df_q_a[df_q_a['content'].notnull()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpJB9TP73W8i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a46c6709-ed77-4535-fe71-64c1a03711f6"
      },
      "source": [
        "df_q_a.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(715378, 18)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPVeXpUt1kUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_q_a['content'] # the features we want to analyze\n",
        "ylabels = df_q_a['is_best_answer'] # the labels, or answers, we want to test against\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJTAcVhb2vPx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "a03b9dad-6082-42bc-c8f8-872a5926e31c"
      },
      "source": [
        "X_train.head"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of 18359     i can not remove it from the control panal... ...\n",
              "171202    \",\"Dear Shams, \\n\\\\n<br />\\n\\\\n<br />While any...\n",
              "188144    \",\"Hello,\\n\\\\nbr />please can you kindly help ...\n",
              "416500                                          it is goood\n",
              "120908    \",\"HI.. I have a problem with CS 1.6 on Vista ...\n",
              "                                ...                        \n",
              "460469    \",\"Check All of your connections. Double check...\n",
              "466879    \",\"Good morning all, \\n\\\\n<br />\\n\\\\n<br />\\n\\...\n",
              "428461    \",\"Hello there. I get these message everytime ...\n",
              "643463    I forget my Facebook old number I remember a p...\n",
              "584774    \",\"Hello, \\n\\\\n<br />\\n\\\\n<br />\\n\\\\n<br />\\n\\...\n",
              "Name: content, Length: 500764, dtype: object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqnBrmNdMC19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbNdd34PMstK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYaX3SKeMg6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViuRZX6p2e1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Logistic Regression Classifier\n",
        "import xgboost as xgb\n",
        "from xgboost import plot_importance, plot_tree\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "#classifier = LogisticRegression()\n",
        "#classifier=xgb.XGBRegressor(n_estimators=1000)\n",
        "classifier=RandomForestClassifier(n_estimators = 1000, random_state = 42)\n",
        "\n",
        "# Create pipeline using Bag of Words\n",
        "pipe = Pipeline([(\"cleaner\", predictors()),\n",
        "                 ('vectorizer', bow_vector),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# model generation\n",
        "pipe.fit(X_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hcwg1TH22kiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "# Predicting with a test dataset\n",
        "predicted = pipe.predict(X_test)\n",
        "\n",
        "# Model Accuracy\n",
        "print(\"RF Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
        "print(\"RF Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
        "print(\"RF Regression Recall:\",metrics.recall_score(y_test, predicted))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQVsZDwuIU4A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "d4de2b1a-ebcd-4077-ab21-cc72ffb5ba57"
      },
      "source": [
        "from sklearn.externals import joblib\n",
        "joblib.dump(pipe, 'pipeline.pkl')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pipeline.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaxJCtbAKJ0w",
        "colab_type": "code",
        "outputId": "018aa1bc-c37f-4a2b-9603-0b03d50897db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "  drive.flush_and_unmount()\n",
        "print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All changes made in this colab session should now be visible in Drive.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPp8UxKO1jgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}